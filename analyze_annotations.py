#!/usr/bin/env python3
"""
æ·±åº¦åˆ†æADE20Kæ ‡æ³¨æ–‡ä»¶ä¸­çš„å¯¹è±¡åç§°
æ‰¾å‡ºä¸ºä»€ä¹ˆæŸäº›ç±»åˆ«è®­ç»ƒä¸åˆ°çš„æ ¹æœ¬åŸå› 
"""

import os
import json
import random
from collections import Counter, defaultdict
from typing import Dict, List, Set
import matplotlib.pyplot as plt
import seaborn as sns

# æˆ‘ä»¬çš„ç›®æ ‡ç±»åˆ«
TARGET_CLASSES = [
    'background', 'clouds', 'person', 'sky', 'hill', 'rock', 'tree', 
    'leaf', 'river', 'lake', 'bush', 'dog', 'cat', 'flower', 'grass', 
    'bird', 'duck'
]

def load_annotation_safe(json_path: str) -> Dict:
    """å®‰å…¨åŠ è½½æ ‡æ³¨æ–‡ä»¶"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except UnicodeDecodeError:
        try:
            with open(json_path, 'r', encoding='latin-1') as f:
                return json.load(f)
        except Exception:
            return {'annotation': {'object': []}}
    except Exception:
        return {'annotation': {'object': []}}

def analyze_annotations(data_dir: str = "ADE20K", max_files: int = 1000):
    """åˆ†ææ ‡æ³¨æ–‡ä»¶ä¸­çš„å¯¹è±¡åç§°"""
    
    print("ğŸ” å¼€å§‹æ·±åº¦åˆ†æADE20Kæ ‡æ³¨æ–‡ä»¶...")
    
    # ç»Ÿè®¡æ•°æ®
    all_object_names = Counter()
    target_related_names = defaultdict(list)  # è®°å½•ä¸ç›®æ ‡ç±»åˆ«ç›¸å…³çš„æ‰€æœ‰åç§°
    sample_files = []  # è®°å½•æ ·æœ¬æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºåç»­è¯¦ç»†æ£€æŸ¥
    total_files = 0
    processed_files = 0
    
    # ADE20Kè®­ç»ƒé›†è·¯å¾„
    training_path = os.path.join(data_dir, "images", "ADE", "training")
    
    if not os.path.exists(training_path):
        print(f"âŒ é”™è¯¯: è·¯å¾„ä¸å­˜åœ¨ {training_path}")
        return
    
    # æ”¶é›†æ‰€æœ‰JSONæ–‡ä»¶
    json_files = []
    for scene_type in os.listdir(training_path):
        scene_type_path = os.path.join(training_path, scene_type)
        if not os.path.isdir(scene_type_path):
            continue
            
        for scene_name in os.listdir(scene_type_path):
            scene_path = os.path.join(scene_type_path, scene_name)
            if not os.path.isdir(scene_path):
                continue
                
            for filename in os.listdir(scene_path):
                if filename.endswith('.json'):
                    json_files.append(os.path.join(scene_path, filename))
    
    total_files = len(json_files)
    print(f"ğŸ“ æ‰¾åˆ° {total_files} ä¸ªæ ‡æ³¨æ–‡ä»¶")
    
    # éšæœºé‡‡æ ·åˆ†æï¼ˆå¦‚æœæ–‡ä»¶å¤ªå¤šï¼‰
    if total_files > max_files:
        json_files = random.sample(json_files, max_files)
        print(f"ğŸ² éšæœºé‡‡æ · {max_files} ä¸ªæ–‡ä»¶è¿›è¡Œåˆ†æ")
    
    # åˆ†ææ¯ä¸ªæ–‡ä»¶
    for json_path in json_files:
        try:
            annotation = load_annotation_safe(json_path)
            objects = annotation.get('annotation', {}).get('object', [])
            
            for obj in objects:
                obj_name = obj.get('name', '').lower().strip()
                if obj_name:
                    all_object_names[obj_name] += 1
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ç›®æ ‡ç±»åˆ«ç›¸å…³
                    for target in TARGET_CLASSES[1:]:  # è·³è¿‡background
                        if (target.lower() in obj_name or 
                            obj_name in target.lower() or 
                            any(keyword in obj_name for keyword in get_keywords(target))):
                            target_related_names[target].append(obj_name)
            
            processed_files += 1
            if processed_files % 100 == 0:
                print(f"â³ å·²å¤„ç† {processed_files}/{len(json_files)} ä¸ªæ–‡ä»¶...")
                
        except Exception as e:
            continue
    
    print(f"âœ… åˆ†æå®Œæˆï¼å¤„ç†äº† {processed_files} ä¸ªæ–‡ä»¶")
    
    # è¾“å‡ºåˆ†æç»“æœ
    print_analysis_results(all_object_names, target_related_names)
    
    # åˆ›å»ºå¯è§†åŒ–
    create_analysis_visualizations(all_object_names, target_related_names)
    
    return all_object_names, target_related_names

def get_keywords(target_class: str) -> List[str]:
    """è·å–ç›®æ ‡ç±»åˆ«çš„ç›¸å…³å…³é”®è¯"""
    keywords_map = {
        'sky': ['sky', 'heaven'],
        'tree': ['tree', 'trees', 'palm', 'pine', 'oak', 'maple', 'birch'],
        'grass': ['grass', 'lawn', 'turf'],
        'river': ['river', 'stream', 'creek', 'brook'],
        'bush': ['bush', 'shrub', 'bushes'],
        'bird': ['bird', 'birds', 'eagle', 'pigeon', 'sparrow', 'crow'],
        'duck': ['duck', 'ducks', 'mallard'],
        'dog': ['dog', 'dogs', 'puppy', 'canine'],
        'cat': ['cat', 'cats', 'kitten', 'feline'],
        'person': ['person', 'people', 'human', 'man', 'woman', 'child'],
        'clouds': ['cloud', 'clouds', 'cloudy'],
        'hill': ['hill', 'hills', 'mound'],
        'rock': ['rock', 'rocks', 'stone', 'stones', 'boulder'],
        'leaf': ['leaf', 'leaves', 'foliage'],
        'lake': ['lake', 'pond', 'reservoir'],
        'flower': ['flower', 'flowers', 'blossom', 'bloom'],
    }
    return keywords_map.get(target_class, [target_class])

def print_analysis_results(all_names: Counter, target_related: Dict):
    """æ‰“å°åˆ†æç»“æœ"""
    
    print(f"\nğŸ“Š æ ‡æ³¨æ–‡ä»¶åˆ†æç»“æœ:")
    print(f"æ€»è®¡å‘ç° {len(all_names)} ç§ä¸åŒçš„å¯¹è±¡åç§°")
    print(f"æ€»è®¡æ ‡æ³¨å¯¹è±¡ {sum(all_names.values())} ä¸ª")
    
    print(f"\nğŸ” æœ€å¸¸è§çš„30ä¸ªå¯¹è±¡åç§°:")
    for name, count in all_names.most_common(30):
        print(f"  {name}: {count:,}")
    
    print(f"\nğŸ¯ ç›®æ ‡ç±»åˆ«ç›¸å…³åç§°åˆ†æ:")
    for target in TARGET_CLASSES[1:]:  # è·³è¿‡background
        related_names = set(target_related.get(target, []))
        if related_names:
            total_count = sum(all_names[name] for name in related_names)
            print(f"  âœ… {target}: æ‰¾åˆ° {len(related_names)} ç§ç›¸å…³åç§°ï¼Œå…± {total_count:,} ä¸ªå¯¹è±¡")
            # æ˜¾ç¤ºæœ€å¸¸è§çš„ç›¸å…³åç§°
            related_counts = [(name, all_names[name]) for name in related_names]
            related_counts.sort(key=lambda x: x[1], reverse=True)
            for name, count in related_counts[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"    - {name}: {count:,}")
        else:
            print(f"  âŒ {target}: æœªæ‰¾åˆ°ç›¸å…³åç§°")
    
    # æ£€æŸ¥å¯èƒ½è¢«é—æ¼çš„ç±»åˆ«
    print(f"\nğŸ” å¯èƒ½ç›¸å…³ä½†æœªåŒ¹é…çš„å¯¹è±¡åç§°:")
    unmatched_names = []
    for name, count in all_names.most_common(100):  # æ£€æŸ¥å‰100ä¸ªå¸¸è§åç§°
        is_matched = False
        for target in TARGET_CLASSES[1:]:
            if name in target_related.get(target, []):
                is_matched = True
                break
        if not is_matched and count > 50:  # åªæ˜¾ç¤ºå‡ºç°é¢‘ç‡è¾ƒé«˜çš„
            unmatched_names.append((name, count))
    
    for name, count in unmatched_names[:20]:  # æ˜¾ç¤ºå‰20ä¸ª
        print(f"  âš ï¸  {name}: {count:,}")

def create_analysis_visualizations(all_names: Counter, target_related: Dict):
    """åˆ›å»ºåˆ†æå¯è§†åŒ–å›¾è¡¨"""
    
    plt.style.use('default')
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. æœ€å¸¸è§å¯¹è±¡åç§°
    ax1 = axes[0, 0]
    top_names = dict(all_names.most_common(15))
    ax1.barh(range(len(top_names)), list(top_names.values()), color='skyblue', alpha=0.7)
    ax1.set_xlabel('Count')
    ax1.set_ylabel('Object Names')
    ax1.set_title('Top 15 Most Common Object Names')
    ax1.set_yticks(range(len(top_names)))
    ax1.set_yticklabels(list(top_names.keys()))
    
    # 2. ç›®æ ‡ç±»åˆ«è¦†ç›–æƒ…å†µ
    ax2 = axes[0, 1]
    target_counts = []
    target_labels = []
    for target in TARGET_CLASSES[1:]:
        related_names = target_related.get(target, [])
        if related_names:
            total_count = sum(all_names[name] for name in related_names)
            target_counts.append(total_count)
        else:
            target_counts.append(0)
        target_labels.append(target)
    
    bars = ax2.bar(range(len(target_labels)), target_counts, color='lightcoral', alpha=0.7)
    ax2.set_xlabel('Target Classes')
    ax2.set_ylabel('Total Object Count')
    ax2.set_title('Target Class Coverage in Annotations')
    ax2.set_xticks(range(len(target_labels)))
    ax2.set_xticklabels(target_labels, rotation=45, ha='right')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, count in zip(bars, target_counts):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + max(target_counts)*0.01,
                f'{count}', ha='center', va='bottom', fontsize=8)
    
    # 3. ç±»åˆ«åŒ¹é…çŠ¶æ€
    ax3 = axes[1, 0]
    matched_count = sum(1 for target in TARGET_CLASSES[1:] if target_related.get(target, []))
    unmatched_count = len(TARGET_CLASSES) - 1 - matched_count
    
    labels = ['Matched', 'Unmatched']
    sizes = [matched_count, unmatched_count]
    colors = ['green', 'red']
    
    ax3.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
    ax3.set_title('Target Class Matching Status')
    
    # 4. å¯¹è±¡åˆ†å¸ƒç›´æ–¹å›¾
    ax4 = axes[1, 1]
    counts = list(all_names.values())
    ax4.hist(counts, bins=50, color='lightgreen', alpha=0.7, edgecolor='black')
    ax4.set_xlabel('Object Count')
    ax4.set_ylabel('Frequency')
    ax4.set_title('Distribution of Object Counts')
    ax4.set_yscale('log')
    
    plt.tight_layout()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs("outputs", exist_ok=True)
    plt.savefig('outputs/annotation_analysis.png', dpi=300, bbox_inches='tight')
    print(f"\nğŸ“ˆ åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: outputs/annotation_analysis.png")

if __name__ == "__main__":
    # åˆ†ææ ‡æ³¨æ–‡ä»¶
    all_names, target_related = analyze_annotations()
    
    print(f"\nğŸ’¡ é—®é¢˜è¯Šæ–­:")
    zero_iou_classes = ['sky', 'tree', 'bush', 'dog', 'grass', 'bird', 'duck']
    
    print(f"  ğŸ“‹ IoUä¸º0çš„ç±»åˆ«åˆ†æ:")
    for cls in zero_iou_classes:
        related_names = target_related.get(cls, [])
        if related_names:
            total_count = sum(all_names[name] for name in set(related_names))
            print(f"    {cls}: æ‰¾åˆ°ç›¸å…³æ ‡æ³¨ {total_count:,} ä¸ª")
        else:
            print(f"    {cls}: âŒ å®Œå…¨æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ ‡æ³¨")
    
    print(f"\nğŸ”§ å»ºè®®çš„ä¿®å¤æ–¹æ¡ˆ:")
    print(f"  1. æ›´æ–°æ•°æ®å¤„ç†é€»è¾‘ä¸­çš„åç§°æ˜ å°„è¡¨")
    print(f"  2. æ·»åŠ æ›´å¤šåŒä¹‰è¯å’Œå˜ä½“åç§°")
    print(f"  3. å®ç°æ›´æ™ºèƒ½çš„åç§°åŒ¹é…ç®—æ³•")
    print(f"  4. æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶çš„å®Œæ•´æ€§å’Œæ ¼å¼")
