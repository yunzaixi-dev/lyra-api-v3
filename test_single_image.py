#!/usr/bin/env python3
"""
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œè¯­ä¹‰åˆ†å‰²æµ‹è¯•
"""

import os
import argparse
import cv2
import torch
import numpy as np
import albumentations as A
from albumentations.pytorch import ToTensorV2
import matplotlib.pyplot as plt
from models.segmentation_model import MaskGenerationModel

# ç›®æ ‡ç±»åˆ«ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
TARGET_CLASSES = [
    'background', 'clouds', 'person', 'sky', 'hill', 'rock', 'tree', 
    'leaf', 'river', 'lake', 'bush', 'dog', 'cat', 'flower', 'grass', 
    'bird', 'duck'
]

# ä¸ºå¯è§†åŒ–åˆ›å»ºä¸€ä¸ªé¢œè‰²æ˜ å°„
COLOR_MAP = plt.cm.get_cmap('tab20', len(TARGET_CLASSES))

def load_model(model_path: str, device: torch.device) -> MaskGenerationModel:
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹æ£€æŸ¥ç‚¹"""
    print(f"åŠ è½½æ¨¡å‹ä»: {model_path}")
    model = MaskGenerationModel(
        architecture='deeplabv3plus',
        encoder_name='resnet50',
        in_channels=3,
        classes=len(TARGET_CLASSES)
    )
    # åŠ è½½çŠ¶æ€å­—å…¸ï¼Œå¹¶å¿½ç•¥ä»»ä½•ä¸åŒ¹é…çš„é”®ï¼ˆä¾‹å¦‚ optimizer stateï¼‰
    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=False)['model_state_dict'], strict=False)
    model.to(device)
    model.eval()
    print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    return model

def preprocess_image(image_path: str, image_size: tuple = (256, 256)) -> torch.Tensor:
    """åŠ è½½å¹¶é¢„å¤„ç†å•å¼ å›¾ç‰‡"""
    image = cv2.imread(image_path)
    if image is None:
        raise FileNotFoundError(f"å›¾ç‰‡æ–‡ä»¶æœªæ‰¾åˆ°: {image_path}")
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    transform = A.Compose([
        A.Resize(image_size[0], image_size[1]),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ])
    
    processed_image = transform(image=image)['image']
    return processed_image.unsqueeze(0) # å¢åŠ  batch ç»´åº¦

def visualize_prediction(image_path: str, prediction: np.ndarray, save_path: str):
    """å¯è§†åŒ–åŸå›¾ã€é¢„æµ‹æ©ç å’Œå åŠ æ•ˆæœ"""
    original_image = cv2.imread(image_path)
    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)
    original_image = cv2.resize(original_image, (prediction.shape[1], prediction.shape[0]))

    # å°†é¢œè‰²æ˜ å°„åº”ç”¨åˆ°é¢„æµ‹æ©ç 
    colored_mask = (COLOR_MAP(prediction / len(TARGET_CLASSES))[:, :, :3] * 255).astype(np.uint8)

    # åˆ›å»ºå åŠ å›¾åƒ
    overlay = cv2.addWeighted(original_image, 0.6, colored_mask, 0.4, 0)

    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    axes[0].imshow(original_image)
    axes[0].set_title('åŸå›¾')
    axes[0].axis('off')

    axes[1].imshow(colored_mask)
    axes[1].set_title('é¢„æµ‹æ©ç ')
    axes[1].axis('off')

    axes[2].imshow(overlay)
    axes[2].set_title('å åŠ æ•ˆæœ')
    axes[2].axis('off')

    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"\nğŸ‰ å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {save_path}")

def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•è¯­ä¹‰åˆ†å‰²æ¨¡å‹")
    parser.add_argument('--model_path', type=str, default='checkpoints/best_model.pth', help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--image_path', type=str, required=True, help='éœ€è¦æµ‹è¯•çš„å›¾ç‰‡è·¯å¾„')
    parser.add_argument('--output_path', type=str, default='outputs/test_result.png', help='è¾“å‡ºç»“æœä¿å­˜è·¯å¾„')
    parser.add_argument('--image_size', type=int, nargs=2, default=[256, 256], help='æ¨¡å‹è¾“å…¥çš„å›¾ç‰‡å°ºå¯¸')
    args = parser.parse_args()

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(args.output_path), exist_ok=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # 1. åŠ è½½æ¨¡å‹
    model = load_model(args.model_path, device)

    # 2. åŠ è½½å¹¶é¢„å¤„ç†å›¾ç‰‡
    input_tensor = preprocess_image(args.image_path, tuple(args.image_size)).to(device)

    # 3. æ‰§è¡Œæ¨ç†
    print("\nğŸš€ å¼€å§‹æ¨ç†...")
    with torch.no_grad():
        output = model(input_tensor)
        # å¯¹äºDeepLabV3+ï¼Œè¾“å‡ºå¯èƒ½åœ¨ 'out' é”®ä¸­
        if isinstance(output, dict):
            logits = output['out']
        else:
            logits = output
        
        # è·å–é¢„æµ‹ç»“æœ
        prediction = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()
    print("æ¨ç†å®Œæˆ!")

    # 4. å¯è§†åŒ–ç»“æœ
    visualize_prediction(args.image_path, prediction, args.output_path)

if __name__ == '__main__':
    main()
